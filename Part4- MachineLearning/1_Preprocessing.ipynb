{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "sG5VeQVV4nKe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "92b20c59-6d33-4fb4-ecab-a9c036150279"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. Data Pre-Processing\\n    1.1 Import the Libraries\\n    1.2 Import the Dataset\\n    1.3 Take care of missing values\\n    1.4 Encode Categorical data - Independent and Dependent Variables\\n    1.5 Splitting the dataset into the Training and Test set\\n    1.6 Feature Scaling \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "'''\n",
        "1. Data Pre-Processing\n",
        "    1.1 Import the Libraries\n",
        "    1.2 Import the Dataset\n",
        "    1.3 Take care of missing values\n",
        "    1.4 Encode Categorical data - Independent and Dependent Variables\n",
        "    1.5 Splitting the dataset into the Training and Test set\n",
        "    1.6 Feature Scaling\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: to import the required libraries\n",
        "\n",
        "import numpy as np\n",
        "# to work with arrays, np is a shortcut, usefull to call later\n",
        "import matplotlib.pyplot  as plt\n",
        "# to draw charts and graphs, pyplot specefically\n",
        "import pandas as pd\n",
        "# to import the dataset and use matrix of features"
      ],
      "metadata": {
        "id": "vqNkLSdr4spc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP2 :is to import the required dataset\n",
        "\n",
        "dataset = pd.read_csv('Data.csv')     # this will create the data frame,\n",
        "#you know, all the values inside this dataset. And this data frame will\n",
        "#be exactly this dataset variable.\n",
        "\n",
        "# A dataset will always have features and dependent varibales(which we are\n",
        "#training and later on used to train the model to predict, always the last\n",
        "#column)\n",
        "\n",
        "X =  dataset.iloc[:, :-1].values\n",
        "# for matrix of features ,iloc stands for locate indexes, : stands for everyrow,\n",
        "#:-1 stands for every column upto last but excluded as the upper bound gets\n",
        "#excluded\n",
        "\n",
        "Y = dataset.iloc[:, -1].values # for the dependent variable vector,\n",
        "#-1 stands for the lsat column\n",
        "\n",
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "5LzmB0es43M_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5198640f-33db-4de8-b540-79c49a09faa0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n",
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP3 :is to take care of missing data, by replacing it with the average\n",
        "\n",
        "df = pd.read_csv('Data.csv')\n",
        "missing_data = df.isnull().sum() # Identify missing data (assumes that missing\n",
        "#data is represented as NaN)\n",
        "print(\"Missing data: \\n\", missing_data) # Print the number of missing entries\n",
        "#in each column\n",
        "\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "# an object created to replace all the missing values with the mean\n",
        "\n",
        "imputer.fit(X[:,1:3]) # fitting that object in our dataset, connecting\n",
        "#our imputer to our matrix of features.\n",
        "\n",
        "X[:,1:3] = imputer.transform(X[:,1:3]) # this transform method will exactly do\n",
        "#that replacement of the missing salary and age here by the mean of the salaries\n",
        "\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aVLObGMdNqo-",
        "outputId": "75054a16-de4e-44c1-cd52-d9d9e8dad6d9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing data: \n",
            " Country      0\n",
            "Age          1\n",
            "Salary       1\n",
            "Purchased    0\n",
            "dtype: int64\n",
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP4 :is to encode categorical data\n",
        "\n",
        "# to make understand the model that there is no relation between the\n",
        "#categorical data, i.e. France, Spain and germany\n",
        "\n",
        "# for that we do \"one hot encoding\", i.e splitting the dataset into\n"
      ],
      "metadata": {
        "id": "U9gamWEqUhN1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4.1 Encoding the independent variable (France, Spain, Germany)\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(), [0] )],\n",
        "                       remainder='passthrough')\n",
        "\n",
        "X = np.array(ct.fit_transform(X))\n",
        "\n",
        "print(X)\n",
        "\n",
        "#France encoded as 1.0.0, Spain as 0.0.1, Gerany as 0.1.0 vector below\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FgPBbKuJWImV",
        "outputId": "84b52ee7-f1e8-49cf-f93a-963f1c094502"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4.2 Encoding the dependent variable  (Yes,No)\n",
        "\n",
        "#use another class called \"Label Encoder\" which will exactly encode these\n",
        "#no's and yes's into zeros and ones.\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le =LabelEncoder()\n",
        "Y = le.fit_transform(Y)\n",
        "\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SgKF10CkX7hc",
        "outputId": "a918a3f8-adeb-43f7-f184-ee7de27ee85f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 Splitting the dataset into the Training and Test set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_test, Y_train = train_test_split(X,Y,test_size=0.2,\n",
        "                                                    random_state=1)\n",
        "\n",
        "print(X_train)\n",
        "print(Y_train)\n",
        "print(X_test)\n",
        "print(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3WjBnayaZr51",
        "outputId": "f94954d8-91cc-42ab-ff8d-466a620be27b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n",
            "[0 1]\n",
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n",
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step6: Feature Scaling\n",
        "\n",
        "'''\n",
        "apply feature scaling after the split, to avoid indeed information leakage\n",
        "because simply the test set is supposed to be something new\n",
        "\n",
        "why use feaure scaling? because for some of the machinery models\n",
        "that's in order to avoid some features to be dominated by other features\n",
        "in such a way that the dominated features are not even considered\n",
        "by the machinery model.\n",
        "\n",
        "Two types of Feature scaling are : Normalization and Standardization\n",
        "\n",
        "Normalization is   x' = (x - x_min) / (x_max - x_min), which will\n",
        "end up in a new column where the values range from 0 to 1\n",
        "\n",
        "Normalization is recommended when you have\n",
        "a normal distribution in most of your features.\n",
        "\n",
        "\n",
        "Standardization is x' = (x- mean) / standard_deviation, which will\n",
        "end up in a new column where the values range from -3 to 3,\n",
        "if outliers present, can go beyond\n",
        "\n",
        "Standardization actually works well ***ALL THE TIME***.\n",
        "\n",
        "'''\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])\n",
        "'''\n",
        "Most frequently asked Q : do we have to apply feature scaling\n",
        "standardization to the dummy variables in the matrix of features?\n",
        "\n",
        "The answer is no, because simply, well remember the goal  of standardization or\n",
        "feature scaling in general, it is to have all the  values of the features\n",
        "in the same range.\n",
        "'''\n",
        "\n",
        "print(X_test)\n",
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A9vVJa2OcBpI",
        "outputId": "f0e763f3-474c-4d0e-d119-1e3db1c27588"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n",
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ]
    }
  ]
}